# Cognitive-HDC-system
*Experimental cognitive architecture based on associative memory and experience-driven learning*

## 0. Briefly: What is it?

This architecture is inspired by the principles of human thought and is designed to be adaptive and learn from its mistakes. This architecture doesn't require datasets for training; it can be trained simply through dialogue, as its essence is to create associative links between vectors.

---

## 1. Why does this project exist?

The current approach to AI is quite expensive and also relies on guessing. I'm not satisfied with the limitations of this approach. Resources are wasted on tokens and datasets. And all this only involves guessing the next words, albeit fascinating. But I would like to create a neural network that can truly "think" about what is said, about what it says itself. And not "think" in the human sense, but process data, but in a way that mimics human methods. My theory is that this approach is more effective in the long term than traditional transformers. And I want to explore whether this is true.

---

## 2. Key Architectural Principles

The central idea of ​​the entire architecture can be summed up in three words: mistakes are good. But beyond that, there are several other points that could be called principles of this project.

- Learning shouldn't simply be through tons of information; it should carry meaning, not probabilities.

- The system doesn't rely on tokens. Each word is a set of associations; it has a specific meaning, not just a number.

- Meaning, as the connections between words, cannot emerge without two things: associative memory and a subjective state.

- Remembering everything = not learning. Lack of learning is unproductive. Forgetting is a necessary process for improvement.

- Mistakes are a way to find direction, not failure. Mistakes are critically important.

- Associative thinking also allows for the discovery of antonymous meanings, allowing the system to make assumptions.

- Experience based on errors can allow the system to compare not only its assumptions with reality but also to learn from new experiences.

---

## 3. General Structure of the System

The information processing cycle through a simulated thinking process goes through the following stages:

1. Receiving information into the system in the form of vectors.
2. Searching for associations with the received information.
3. Focusing on the key point of the received information.
4. Generating various possible scenarios based on the received information.
5. Critically evaluating all previous points based on the system's experience.
6. Consolidating selected data into memory.
7. Generating a response or remaining silent.

In the classical sensory sense, the architecture doesn't have perception as such, as it immediately operates on meanings, skipping the stage of empty sensations. It does simulate sensations, necessary for the formation of meanings more fully than just text, but they are always fully integrated with one another; the architecture doesn't single out any one thing, being capable of processing everything at once.

The system's expectations are a selection of the most probable outcomes based on associations, which will ultimately be compared with reality. This is reminiscent of transformer-based models in this regard, but instead of a simple system of calculating statistics for probability, everything depends on the architecture's unique experience, which makes processing more complex and experience-dependent.

The system's experience is the connections between objects in its memory. And an object can be not only a word but also anything else, depending on the implementation. But the level of words is the most convenient place to start.

Errors are a comparison of the system's expectations of what is happening right now and what is actually happening. This is not an attempt to predict the future, but an attempt to "understand" the present.

---

## 4. Memory

In this case, memory is not simply the storage of data, but a mechanism for selecting and constantly modifying experience.

The architecture contains multiple types of memory, each responsible for its own area. This fact speaks to the complexity and multifaceted nature of human memory. Storing information without specific processing is comparable to an encyclopedia, not experience. The system's memory types can be generally described as follows:

- Short-term and working memory, areas responsible for processing the present moment at two levels.

- Long-term memory, as the main repository of concepts, the primary knowledge base, which has been proven by experience to be valuable to the architecture and not just noise.

- Narrative memory, as a way to store cause-and-effect relationships.

- Social memory, for assessing the reliability of information sources and distinguishing between them.

- Memory of mistakes. The only way to avoid repeating mistakes is to remember them.

- Metamemory, a layer that remembers its own actions to correct itself in the future.

---

## 5. Training without datasets

An architecture shouldn't be trained using datasets if data quality is important. So, dialogue is the best way to teach this system anything. By storing information in vectors and simulating meanings, the system can learn any language by understanding its patterns. And in this architecture, datasets are unlikely to form better connections between objects in this system than natural explanations through dialogue. Learning through datasets is also possible, but this artificially substitutes meanings, which reduces efficiency.

---

## 6. Counterfactuals and Alternatives

By selecting strange, disparate vectors of meanings, the system can compare them with its own experience and thus form new experiences without external intervention. This information processing allows for increased learning efficiency, not simply by reacting to received data, but also by attempting to form new ones, still relying on what is already known to verify its validity. This brings the architecture closer to the ability to reason, not just react.

---

## 7. Logic and Mathematics (plans)

An experiment is planned where the system will learn mathematics based on how its mind works. It's almost impossible to achieve results using the architecture in its pure form, but it's hypothesized that if you provide a separate view of operators and numbers, as well as tens of thousands of examples, a pattern will be detected. This is especially true if you allow the system to make assumptions without providing a clear answer, forcing it to make mistakes.

It's important to note that the experiment's success is not guaranteed, which is why we plan to test its feasibility.

---

## 8. What the system can't and doesn't do

This architecture doesn't rely on LLMs, even for answer generation; that would be a cumbersome workaround. Furthermore, there's a list of things the system doesn't include:

1. The architecture isn't designed for learning through datasets.
2. The system is aimed at learning narrow topics, not knowledge of everything at once.
3. The architecture is designed to learn, and therefore, it will make mistakes.
4. This project does not replicate the human brain or psyche, but human thinking was the basis for this architecture.
5. This idea does not aim to imitate consciousness or anything else, only the thought process.

---

## 9. Project Status

At present, a third prototype is being created to improve on previous shortcomings. The first prototype was an attempt to test the feasibility of this idea, and it was successful. The second prototype was intended as a final test of the system, but I forgot an important detail, which is why an improved third prototype is now being created.

---

## 10. Who is this project for?

This project is not for those who need immediate results, as it is a learning adaptive architecture, and learning takes time. The project should not be expected to provide results it is not designed for.

---

## 11. License / Contribution

**PolyForm Noncommercial 1.0.0**
